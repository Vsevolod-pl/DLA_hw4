{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a1187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import os\n",
    "import json\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "\n",
    "import wav2mel\n",
    "from dataset import WMDataset, collate_fn\n",
    "from models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator, feature_loss, discriminator_loss, generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0394abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspectrogrammator_config = wav2mel.MelSpectrogramConfig()\n",
    "melspectrogrammator = wav2mel.MelSpectrogram(melspectrogrammator_config)\n",
    "melspectrogrammator_cuda = wav2mel.MelSpectrogram(melspectrogrammator_config).cuda()\n",
    "\n",
    "path2wavs = './wavs/'\n",
    "random_crop = True\n",
    "dataset = WMDataset(path2wavs=path2wavs, random_crop=random_crop, melspec_config=melspectrogrammator_config)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e426bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config_v2.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3cabf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiScaleDiscriminator(\n",
       "  (discriminators): ModuleList(\n",
       "    (0): DiscriminatorS(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(1, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(41,), stride=(2,), padding=(20,), groups=4)\n",
       "        (2): Conv1d(128, 256, kernel_size=(41,), stride=(2,), padding=(20,), groups=16)\n",
       "        (3): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (4): Conv1d(512, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (5): Conv1d(1024, 1024, kernel_size=(41,), stride=(1,), padding=(20,), groups=16)\n",
       "        (6): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (1): DiscriminatorS(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(1, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(41,), stride=(2,), padding=(20,), groups=4)\n",
       "        (2): Conv1d(128, 256, kernel_size=(41,), stride=(2,), padding=(20,), groups=16)\n",
       "        (3): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (4): Conv1d(512, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (5): Conv1d(1024, 1024, kernel_size=(41,), stride=(1,), padding=(20,), groups=16)\n",
       "        (6): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (2): DiscriminatorS(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(1, 128, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(41,), stride=(2,), padding=(20,), groups=4)\n",
       "        (2): Conv1d(128, 256, kernel_size=(41,), stride=(2,), padding=(20,), groups=16)\n",
       "        (3): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (4): Conv1d(512, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)\n",
       "        (5): Conv1d(1024, 1024, kernel_size=(41,), stride=(1,), padding=(20,), groups=16)\n",
       "        (6): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (conv_post): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (meanpools): ModuleList(\n",
       "    (0): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(2,))\n",
       "    (1): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(2,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(**params).to(device)\n",
    "\n",
    "mpd = MultiPeriodDiscriminator().to(device)\n",
    "msd = MultiScaleDiscriminator().to(device)\n",
    "\n",
    "gen.train()\n",
    "mpd.train()\n",
    "msd.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7211d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g = torch.optim.AdamW(gen.parameters(), params['learning_rate'], betas=[params['adam_b1'], params['adam_b2']])\n",
    "opt_d = torch.optim.AdamW(chain(mpd.parameters(), msd.parameters()), params['learning_rate'], betas=[params['adam_b1'], params['adam_b2']])\n",
    "\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(opt_g, gamma=params[\"lr_decay\"], last_epoch=-1)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(opt_d, gamma=params[\"lr_decay\"], last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eaf9650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvsevolodpl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\vsevo\\Desktop\\vuz\\DLA\\DLA_hw4\\wandb\\run-20221225_232042-j2nsaqrr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vsevolodpl/GAN_vocoder_project/runs/j2nsaqrr\" target=\"_blank\">proud-water-4</a></strong> to <a href=\"https://wandb.ai/vsevolodpl/GAN_vocoder_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/vsevolodpl/GAN_vocoder_project/runs/j2nsaqrr?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c39f0b7040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"GAN_vocoder_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd13a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4a3fcbb81e41b6bb470d76885933e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764911b64f9c43d7923f1762fd23fe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in trange(20):\n",
    "    for batch in tqdm(training_loader):\n",
    "        audio = batch['audio'].to(device)\n",
    "        mel = batch['mel'].to(device)\n",
    "\n",
    "        audio_pred = gen(mel)\n",
    "        mel_pred = melspectrogrammator_cuda(audio_pred.squeeze(1))\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        y_pdisc_real, y_pdisc_generated, _, _ = mpd(audio.unsqueeze(1), audio_pred.detach())\n",
    "        loss_pdisc, losses_pdisc_real, losses_pdisc_generated = discriminator_loss(y_pdisc_real, y_pdisc_generated)\n",
    "\n",
    "        y_scdisc_real, y_scdisc_generated, _, _ = msd(audio.unsqueeze(1), audio_pred.detach())\n",
    "        loss_scdisc, losses_scdisc_real, losses_scdisc_generated = discriminator_loss(y_scdisc_real, y_scdisc_generated)\n",
    "\n",
    "        loss_disc_all = loss_scdisc + loss_pdisc\n",
    "\n",
    "        wandb.log({'Period discriminator loss': loss_pdisc.item()})\n",
    "        wandb.log({'Scale discriminator loss': loss_scdisc.item()})\n",
    "\n",
    "        loss_disc_all.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "\n",
    "        loss_mel = F.l1_loss(mel, mel_pred) * 45\n",
    "\n",
    "        y_pdisc_real, y_pdisc_generated, featmap_p_real, featmap_p_generated = mpd(audio.unsqueeze(1), audio_pred)\n",
    "        y_scdisc_real, y_scdisc_generated, featmap_sc_real, featmap_sc_generated = msd(audio.unsqueeze(1), audio_pred)\n",
    "        loss_featmap_p = feature_loss(featmap_p_real, featmap_p_generated)\n",
    "        loss_featmap_sc = feature_loss(featmap_sc_real, featmap_sc_generated)\n",
    "        loss_gen_p, losses_gen_p = generator_loss(y_pdisc_generated)\n",
    "        loss_gen_sc, losses_gen_sc = generator_loss(y_scdisc_generated)\n",
    "        loss_gen_all = loss_gen_sc + loss_gen_p + loss_featmap_sc + loss_featmap_p + loss_mel\n",
    "\n",
    "        wandb.log({'Generator scale loss': loss_gen_sc.item()})\n",
    "        wandb.log({'Generator period loss': loss_gen_p.item()})\n",
    "        wandb.log({'Generator scale feature loss': loss_featmap_sc.item()})\n",
    "        wandb.log({'Generator period feature loss': loss_featmap_p.item()})\n",
    "        wandb.log({'Generator mel loss': loss_mel.item()})\n",
    "\n",
    "        loss_gen_all.backward()\n",
    "        opt_g.step()\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    \n",
    "    if epoch%5 == 4:\n",
    "        torch.save(gen.state_dict(), f'./weights/try2/gen_epoch_{epoch}')\n",
    "\n",
    "        torch.save({'gen': gen.state_dict(),\n",
    "                    'mpd': mpd.state_dict(),\n",
    "                    'msd': msd.state_dict(),\n",
    "                    'opt_g': opt_g.state_dict(),\n",
    "                    'opt_d': opt_d.state_dict(),\n",
    "                    'config': params}, f'./weights/try2/ckpt_epoch_{epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d780ab3",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879e742a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.load_state_dict(torch.load('./weights/try2/gen_epoch_9'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c5ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,4):\n",
    "    audio, sr = torchaudio.load(f'./test_wavs/audio_{j}.wav')\n",
    "    mel_p = melspectrogrammator(audio)\n",
    "    with torch.no_grad():\n",
    "        synth = []\n",
    "        for i in range(0, mel_p.size(-1), 32):\n",
    "            synth.append(gen(mel_p[:,:,i:i+32].cuda()))\n",
    "    result = torch.cat(synth, dim=-1)\n",
    "    torchaudio.save(f'./test_wavs/res_{j}.wav', result[0].detach().cpu(), sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
